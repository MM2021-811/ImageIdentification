{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "%matplotlib inline\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170499072it [00:03, 54133956.52it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/cifar-10-python.tar.gz to ../data/\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#creating a dinstinct transform class for the train, validation and test dataset\n",
    "tranform_train = transforms.Compose([transforms.Resize((227,227)), transforms.RandomHorizontalFlip(p=0.7), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "tranform_test = transforms.Compose([transforms.Resize((227,227)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "#preparing the train, validation and test dataset\n",
    "torch.manual_seed(43)\n",
    "train_ds = CIFAR10(\"../data/\", train=True, download=True, transform=tranform_train) #40,000 original images + transforms\n",
    "val_size = 10000 #there are 10,000 test images and since there are no transforms performed on the test, we keep the validation as 10,000\n",
    "train_size = len(train_ds) - val_size\n",
    "train_ds, val_ds = random_split(train_ds, [train_size, val_size]) #Extracting the 10,000 validation images from the train set\n",
    "test_ds = CIFAR10(\"../data/\", train=False, download=True, transform=tranform_test) #10,000 images\n",
    "\n",
    "#passing the train, val and test datasets to the dataloader\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels= 96, kernel_size= 11, stride=4, padding=0 )\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride= 1, padding= 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride= 1, padding= 1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1  = nn.Linear(in_features= 9216, out_features= 4096)\n",
    "        self.fc2  = nn.Linear(in_features= 4096, out_features= 4096)\n",
    "        self.fc3 = nn.Linear(in_features=4096 , out_features=10)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0 :::: 1.5087065321922302\n",
      "Got 5792 / 10000 with accuracy 57.92\n",
      "Loss in epoch 1 :::: 1.0084469940185548\n",
      "Got 6817 / 10000 with accuracy 68.17\n",
      "Loss in epoch 2 :::: 0.7762707249164581\n",
      "Got 7374 / 10000 with accuracy 73.74\n",
      "Loss in epoch 3 :::: 0.6309053025245667\n",
      "Got 7556 / 10000 with accuracy 75.56\n",
      "Loss in epoch 4 :::: 0.5258161451101303\n",
      "Got 7799 / 10000 with accuracy 77.99\n",
      "Loss in epoch 5 :::: 0.42619119255542753\n",
      "Got 7956 / 10000 with accuracy 79.56\n",
      "Loss in epoch 6 :::: 0.34776435735225675\n",
      "Got 8098 / 10000 with accuracy 80.98\n",
      "Loss in epoch 7 :::: 0.2803079991161823\n",
      "Got 8104 / 10000 with accuracy 81.04\n",
      "Loss in epoch 8 :::: 0.22488376423716544\n",
      "Got 8102 / 10000 with accuracy 81.02\n",
      "Loss in epoch 9 :::: 0.18303570322841406\n",
      "Got 8121 / 10000 with accuracy 81.21\n",
      "Loss in epoch 10 :::: 0.14718485325574876\n",
      "Got 8236 / 10000 with accuracy 82.36\n",
      "Loss in epoch 11 :::: 0.1221895422488451\n",
      "Got 8142 / 10000 with accuracy 81.42\n",
      "Loss in epoch 12 :::: 0.1021112922206521\n",
      "Got 8247 / 10000 with accuracy 82.47\n",
      "Loss in epoch 13 :::: 0.09313451016582548\n",
      "Got 8261 / 10000 with accuracy 82.61\n",
      "Loss in epoch 14 :::: 0.08189197485744953\n",
      "Got 8264 / 10000 with accuracy 82.64\n",
      "Loss in epoch 15 :::: 0.07788092375397682\n",
      "Got 8228 / 10000 with accuracy 82.28\n",
      "Loss in epoch 16 :::: 0.06509327736832202\n",
      "Got 8184 / 10000 with accuracy 81.84\n",
      "Loss in epoch 17 :::: 0.0587806549590081\n",
      "Got 8196 / 10000 with accuracy 81.96\n",
      "Loss in epoch 18 :::: 0.06289752231054008\n",
      "Got 8211 / 10000 with accuracy 82.11\n",
      "Loss in epoch 19 :::: 0.050554938427358864\n",
      "Got 8166 / 10000 with accuracy 81.66\n",
      "Loss in epoch 20 :::: 0.05416171665750444\n",
      "Got 8217 / 10000 with accuracy 82.17\n",
      "Loss in epoch 21 :::: 0.047899899951461704\n",
      "Got 8210 / 10000 with accuracy 82.10\n",
      "Loss in epoch 22 :::: 0.04373037854335271\n",
      "Got 8278 / 10000 with accuracy 82.78\n",
      "Loss in epoch 23 :::: 0.043076485369633884\n",
      "Got 8138 / 10000 with accuracy 81.38\n",
      "Loss in epoch 24 :::: 0.04362247575689107\n",
      "Got 8279 / 10000 with accuracy 82.79\n",
      "Loss in epoch 25 :::: 0.03401904959557578\n",
      "Got 8110 / 10000 with accuracy 81.10\n",
      "Loss in epoch 26 :::: 0.04054528846419416\n",
      "Got 8253 / 10000 with accuracy 82.53\n",
      "Loss in epoch 27 :::: 0.038964198876731096\n",
      "Got 8207 / 10000 with accuracy 82.07\n",
      "Loss in epoch 28 :::: 0.03316000233343802\n",
      "Got 8242 / 10000 with accuracy 82.42\n",
      "Loss in epoch 29 :::: 0.03374170431029051\n",
      "Got 8250 / 10000 with accuracy 82.50\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #training with either cpu or cuda\n",
    "\n",
    "model = AlexNet() #to compile the model\n",
    "model = model.to(device=device) #to send the model for training on either cuda or cpu\n",
    "\n",
    "## Loss and optimizer\n",
    "learning_rate = 1e-4 #I picked this because it seems to be the most used by experts\n",
    "load_model = True\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= learning_rate) #Adam seems to be the most popular for deep learning\n",
    "\n",
    "for epoch in range(20): #I decided to train the model for 50 epochs\n",
    "    loss_ep = 0\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(train_dl):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        ## Forward Pass\n",
    "        optimizer.zero_grad()\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores,targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_ep += loss.item()\n",
    "    print(f\"Loss in epoch {epoch} :::: {loss_ep/len(train_dl)}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        for batch_idx, (data,targets) in enumerate(val_dl):\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "            ## Forward Pass\n",
    "            scores = model(data)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == targets).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        print(\n",
    "            f\"{epoch}: Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100:.2f}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4508d0e2e0be84e90ff361a3824194c554edc39894d10f09a30d81ec9c033e6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
